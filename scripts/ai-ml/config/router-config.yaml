# Advanced Model Router Configuration
# Production-ready routing with intelligent fallbacks, load balancing, and cost optimization
# Version: 2.0.0

# =============================================================================
# ROUTING STRATEGIES
# =============================================================================

routing:
  # Default strategy for all requests
  default_strategy: cost-optimized

  strategies:
    # Cost-optimized routing - minimizes API costs
    cost-optimized:
      type: cost-based
      description: Routes to cheapest model that meets requirements
      priority_order:
        - local   # Free models first
        - cheap   # Low-cost cloud models
        - premium # High-quality fallback
      fallback_on_error: true
      max_retries: 3

    # Quality-optimized routing - maximizes response quality
    quality-optimized:
      type: quality-based
      description: Routes to highest quality model available
      priority_order:
        - premium
        - standard
        - local
      fallback_on_error: true
      max_retries: 2

    # Latency-optimized routing - minimizes response time
    latency-optimized:
      type: latency-based
      description: Routes to fastest responding model
      priority_order:
        - local
        - fast-cloud
        - standard
      timeout_ms: 10000
      fallback_on_timeout: true

    # Round-robin routing - distributes load evenly
    round-robin:
      type: load-balanced
      description: Distributes requests across all healthy models
      health_check_interval: 30
      exclude_unhealthy: true

# =============================================================================
# MODEL TIERS FOR ROUTING
# =============================================================================

model_tiers:
  local:
    description: Self-hosted models with no API costs
    models:
      - llama3.2
      - codellama
      - deepseek-coder-v2
      - qwen2.5-coder
      - mistral
      - llama3.2-vision
    cost_per_1k_tokens: 0
    average_latency_ms: 100
    quality_score: 7

  cheap:
    description: Low-cost cloud API models
    models:
      - gpt-4o-mini
      - claude-3-5-haiku
      - gemini-1.5-flash
    cost_per_1k_tokens: 0.5
    average_latency_ms: 500
    quality_score: 8

  standard:
    description: Standard cloud API models
    models:
      - gpt-4o
      - claude-3-5-sonnet
      - gemini-1.5-pro
    cost_per_1k_tokens: 5
    average_latency_ms: 1000
    quality_score: 9

  premium:
    description: Premium/reasoning models
    models:
      - claude-opus-4
      - claude-sonnet-4
      - o1
      - o3-mini
    cost_per_1k_tokens: 20
    average_latency_ms: 5000
    quality_score: 10

  fast-cloud:
    description: Fast cloud models for low-latency requirements
    models:
      - gpt-4o-mini
      - claude-3-5-haiku
      - gemini-1.5-flash
    cost_per_1k_tokens: 0.5
    average_latency_ms: 300
    quality_score: 8

# =============================================================================
# USE CASE ROUTING RULES
# =============================================================================

use_case_routing:
  # Code generation and analysis
  code_generation:
    primary_models:
      - claude-sonnet-4
      - deepseek-coder-v2
      - qwen2.5-coder
    fallback_models:
      - gpt-4o
      - codellama
    routing_strategy: quality-optimized
    required_capabilities:
      - code_completion
      - syntax_highlighting
    max_tokens: 16384

  # Code review
  code_review:
    primary_models:
      - claude-sonnet-4
      - claude-opus-4
    fallback_models:
      - gpt-4o
      - deepseek-coder-v2
    routing_strategy: quality-optimized
    max_tokens: 32000

  # Chat/conversation
  general_chat:
    primary_models:
      - llama3.2
      - gpt-4o-mini
    fallback_models:
      - claude-3-5-haiku
      - gemini-1.5-flash
    routing_strategy: cost-optimized
    max_tokens: 4096

  # Document analysis
  document_analysis:
    primary_models:
      - claude-sonnet-4
      - gpt-4o
    fallback_models:
      - gemini-1.5-pro
      - llama3.2
    routing_strategy: quality-optimized
    max_tokens: 100000

  # Image understanding
  vision_tasks:
    primary_models:
      - gpt-4o
      - claude-sonnet-4
    fallback_models:
      - gemini-1.5-pro
      - llama3.2-vision
    routing_strategy: quality-optimized
    required_capabilities:
      - vision

  # Complex reasoning
  reasoning:
    primary_models:
      - o3-mini
      - claude-opus-4
    fallback_models:
      - o1
      - gpt-4o
    routing_strategy: quality-optimized
    max_tokens: 32000

  # Embeddings
  embeddings:
    primary_models:
      - text-embedding-3-large
    fallback_models:
      - text-embedding-3-small
      - nomic-embed
      - mxbai-embed
    routing_strategy: cost-optimized

  # Translation
  translation:
    primary_models:
      - gpt-4o
      - claude-sonnet-4
    fallback_models:
      - gemini-1.5-pro
      - llama3.2
    routing_strategy: cost-optimized

  # Summarization
  summarization:
    primary_models:
      - claude-3-5-haiku
      - gpt-4o-mini
    fallback_models:
      - gemini-1.5-flash
      - llama3.2
    routing_strategy: cost-optimized
    max_tokens: 8192

# =============================================================================
# FALLBACK CONFIGURATION
# =============================================================================

fallback_config:
  # Enable automatic fallbacks
  enabled: true

  # Maximum number of fallback attempts
  max_attempts: 3

  # Delay between fallback attempts (ms)
  retry_delay_ms: 1000

  # Exponential backoff multiplier
  backoff_multiplier: 2

  # Conditions that trigger fallback
  fallback_triggers:
    - error_code: 429  # Rate limit
      action: fallback
      delay_ms: 5000
    - error_code: 500  # Server error
      action: retry
      max_retries: 2
    - error_code: 503  # Service unavailable
      action: fallback
      delay_ms: 10000
    - error_code: timeout
      action: fallback
      delay_ms: 0
    - error_code: context_length_exceeded
      action: route_to_longer_context
    - error_code: content_filter
      action: fallback

  # Model-specific fallback chains
  fallback_chains:
    gpt-4o:
      - claude-sonnet-4
      - gemini-1.5-pro
      - llama3.2
    claude-sonnet-4:
      - gpt-4o
      - gemini-1.5-pro
      - deepseek-coder-v2
    gpt-4o-mini:
      - claude-3-5-haiku
      - gemini-1.5-flash
      - llama3.2
    o3-mini:
      - o1-mini
      - claude-opus-4
      - gpt-4o

# =============================================================================
# HEALTH CHECK CONFIGURATION
# =============================================================================

health_checks:
  enabled: true
  interval_seconds: 30

  # Health check endpoint for local models
  local_health_endpoint: /api/health

  # Thresholds for marking model as unhealthy
  unhealthy_thresholds:
    consecutive_failures: 3
    error_rate_percent: 50
    latency_p99_ms: 30000

  # Recovery configuration
  recovery:
    check_interval_seconds: 60
    consecutive_successes_required: 2

  # Per-model health check configuration
  model_health:
    ollama:
      endpoint: http://localhost:11434/api/tags
      timeout_ms: 5000
      expected_status: 200

    openai:
      endpoint: https://api.openai.com/v1/models
      timeout_ms: 10000
      expected_status: 200
      headers:
        Authorization: Bearer ${OPENAI_API_KEY}

    anthropic:
      endpoint: https://api.anthropic.com/v1/models
      timeout_ms: 10000
      expected_status: 200
      headers:
        x-api-key: ${ANTHROPIC_API_KEY}

# =============================================================================
# LOAD BALANCING CONFIGURATION
# =============================================================================

load_balancing:
  enabled: true

  # Algorithm: round-robin, weighted, least-connections, adaptive
  algorithm: adaptive

  # Weights for adaptive load balancing
  adaptive_weights:
    latency: 0.3
    error_rate: 0.3
    cost: 0.2
    queue_depth: 0.2

  # Connection pool settings
  connection_pool:
    max_connections_per_model: 100
    min_connections_per_model: 10
    connection_timeout_ms: 5000
    idle_timeout_ms: 60000

  # Queue settings
  request_queue:
    max_size: 1000
    timeout_ms: 30000
    priority_levels: 3

# =============================================================================
# RATE LIMITING
# =============================================================================

rate_limiting:
  enabled: true

  # Global rate limits
  global:
    requests_per_minute: 1000
    tokens_per_minute: 500000

  # Per-user rate limits
  per_user:
    requests_per_minute: 60
    tokens_per_minute: 100000

  # Per-model rate limits
  per_model:
    gpt-4o:
      requests_per_minute: 100
      tokens_per_minute: 150000
    claude-sonnet-4:
      requests_per_minute: 100
      tokens_per_minute: 200000
    llama3.2:
      requests_per_minute: 500  # Higher for local
      tokens_per_minute: 1000000

  # Rate limit response
  on_limit_exceeded:
    action: queue  # queue, reject, or fallback
    queue_timeout_ms: 30000
    fallback_to_local: true

# =============================================================================
# COST MANAGEMENT
# =============================================================================

cost_management:
  enabled: true

  # Budget configuration
  budgets:
    daily:
      amount: 50
      currency: USD
      action_on_exceed: fallback_to_local
    monthly:
      amount: 500
      currency: USD
      action_on_exceed: alert_and_fallback

  # Cost alerts
  alerts:
    - threshold_percent: 50
      notify: slack
    - threshold_percent: 75
      notify: slack
    - threshold_percent: 90
      notify: slack
      action: switch_to_local
    - threshold_percent: 100
      notify: slack
      action: local_only

  # Cost optimization rules
  optimization_rules:
    - condition: request_length < 1000
      prefer_models:
        - gpt-4o-mini
        - claude-3-5-haiku
        - llama3.2
    - condition: time_of_day in ["00:00-08:00"]
      prefer_models:
        - local  # Use local during off-hours
    - condition: user_tier == "free"
      prefer_models:
        - local
        - gpt-4o-mini

# =============================================================================
# CACHING CONFIGURATION
# =============================================================================

caching:
  enabled: true
  backend: redis

  redis:
    host: ${REDIS_HOST:-localhost}
    port: ${REDIS_PORT:-6379}
    password: ${REDIS_PASSWORD}
    db: 0
    key_prefix: "litellm:cache:"

  # Cache settings
  settings:
    default_ttl_seconds: 3600
    max_cache_size_mb: 1024
    eviction_policy: lru

  # What to cache
  cache_rules:
    - type: embedding
      ttl_seconds: 86400  # 24 hours for embeddings
      enabled: true
    - type: completion
      ttl_seconds: 3600
      enabled: true
      match_threshold: 0.95  # Semantic similarity for cache hits
    - type: chat
      ttl_seconds: 1800
      enabled: false  # Usually don't cache chat

# =============================================================================
# OBSERVABILITY
# =============================================================================

observability:
  # Metrics
  metrics:
    enabled: true
    backend: prometheus
    port: 9090
    path: /metrics

    collect:
      - request_count
      - request_duration
      - token_usage
      - error_count
      - cache_hit_rate
      - model_latency
      - cost_per_request

  # Tracing
  tracing:
    enabled: true
    backend: jaeger
    endpoint: ${JAEGER_ENDPOINT:-http://localhost:14268/api/traces}
    sample_rate: 0.1

  # Logging
  logging:
    level: INFO
    format: json
    include_request_body: false
    include_response_body: false
    mask_api_keys: true

  # Langfuse integration
  langfuse:
    enabled: true
    public_key: ${LANGFUSE_PUBLIC_KEY}
    secret_key: ${LANGFUSE_SECRET_KEY}
    host: ${LANGFUSE_HOST:-https://cloud.langfuse.com}
    flush_interval_seconds: 10
